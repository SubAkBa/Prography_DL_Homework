{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0 \tcost : 15.86425 \tbias : [0.05607617] \tweight : [[-0.66408455]]\n",
      "step : 20 \tcost : 0.20035125 \tbias : [0.5717581] \tweight : [[0.5923785]]\n",
      "step : 40 \tcost : 0.053143997 \tbias : [0.5942415] \tweight : [[0.7237281]]\n",
      "step : 60 \tcost : 0.047098298 \tbias : [0.5710139] \tweight : [[0.74739474]]\n",
      "step : 80 \tcost : 0.042764813 \tbias : [0.5446258] \tweight : [[0.7602834]]\n",
      "step : 100 \tcost : 0.038839594 \tbias : [0.51907295] \tweight : [[0.7716461]]\n",
      "step : 120 \tcost : 0.035274763 \tbias : [0.49468252] \tweight : [[0.7823871]]\n",
      "step : 140 \tcost : 0.03203709 \tbias : [0.47143465] \tweight : [[0.792615]]\n",
      "step : 160 \tcost : 0.029096607 \tbias : [0.44927898] \tweight : [[0.8023614]]\n",
      "step : 180 \tcost : 0.02642598 \tbias : [0.42816448] \tweight : [[0.81164974]]\n",
      "step : 200 \tcost : 0.024000496 \tbias : [0.40804225] \tweight : [[0.8205015]]\n",
      "step : 220 \tcost : 0.021797627 \tbias : [0.38886574] \tweight : [[0.8289373]]\n",
      "step : 240 \tcost : 0.01979696 \tbias : [0.37059048] \tweight : [[0.8369767]]\n",
      "step : 260 \tcost : 0.017979907 \tbias : [0.3531741] \tweight : [[0.84463805]]\n",
      "step : 280 \tcost : 0.016329652 \tbias : [0.33657622] \tweight : [[0.85193956]]\n",
      "step : 300 \tcost : 0.014830852 \tbias : [0.3207584] \tweight : [[0.85889786]]\n",
      "step : 320 \tcost : 0.013469601 \tbias : [0.3056839] \tweight : [[0.8655291]]\n",
      "step : 340 \tcost : 0.0122333225 \tbias : [0.29131788] \tweight : [[0.87184876]]\n",
      "step : 360 \tcost : 0.011110494 \tbias : [0.27762702] \tweight : [[0.8778714]]\n",
      "step : 380 \tcost : 0.010090737 \tbias : [0.26457956] \tweight : [[0.88361096]]\n",
      "step : 400 \tcost : 0.009164562 \tbias : [0.2521453] \tweight : [[0.8890809]]\n",
      "step : 420 \tcost : 0.008323396 \tbias : [0.24029532] \tweight : [[0.8942937]]\n",
      "step : 440 \tcost : 0.007559442 \tbias : [0.2290023] \tweight : [[0.89926153]]\n",
      "step : 460 \tcost : 0.006865607 \tbias : [0.21824004] \tweight : [[0.9039958]]\n",
      "step : 480 \tcost : 0.006235455 \tbias : [0.20798357] \tweight : [[0.90850776]]\n",
      "step : 500 \tcost : 0.0056631295 \tbias : [0.19820909] \tweight : [[0.9128075]]\n",
      "step : 520 \tcost : 0.005143344 \tbias : [0.18889399] \tweight : [[0.91690516]]\n",
      "step : 540 \tcost : 0.0046712747 \tbias : [0.18001671] \tweight : [[0.92081034]]\n",
      "step : 560 \tcost : 0.0042425278 \tbias : [0.17155658] \tweight : [[0.92453194]]\n",
      "step : 580 \tcost : 0.0038531402 \tbias : [0.16349408] \tweight : [[0.9280788]]\n",
      "step : 600 \tcost : 0.0034994702 \tbias : [0.15581043] \tweight : [[0.9314588]]\n",
      "step : 620 \tcost : 0.0031782817 \tbias : [0.14848791] \tweight : [[0.9346799]]\n",
      "step : 640 \tcost : 0.0028865654 \tbias : [0.14150953] \tweight : [[0.93774974]]\n",
      "step : 660 \tcost : 0.0026216197 \tbias : [0.13485906] \tweight : [[0.9406753]]\n",
      "step : 680 \tcost : 0.0023809918 \tbias : [0.12852114] \tweight : [[0.9434633]]\n",
      "step : 700 \tcost : 0.0021624665 \tbias : [0.12248113] \tweight : [[0.9461203]]\n",
      "step : 720 \tcost : 0.001963982 \tbias : [0.11672498] \tweight : [[0.94865245]]\n",
      "step : 740 \tcost : 0.0017837202 \tbias : [0.11123934] \tweight : [[0.95106566]]\n",
      "step : 760 \tcost : 0.0016200019 \tbias : [0.10601148] \tweight : [[0.9533653]]\n",
      "step : 780 \tcost : 0.0014713093 \tbias : [0.10102937] \tweight : [[0.955557]]\n",
      "step : 800 \tcost : 0.0013362723 \tbias : [0.09628137] \tweight : [[0.95764565]]\n",
      "step : 820 \tcost : 0.0012136172 \tbias : [0.09175647] \tweight : [[0.9596363]]\n",
      "step : 840 \tcost : 0.001102231 \tbias : [0.0874442] \tweight : [[0.9615332]]\n",
      "step : 860 \tcost : 0.0010010592 \tbias : [0.08333462] \tweight : [[0.963341]]\n",
      "step : 880 \tcost : 0.0009091801 \tbias : [0.07941822] \tweight : [[0.96506375]]\n",
      "step : 900 \tcost : 0.00082573376 \tbias : [0.07568585] \tweight : [[0.9667057]]\n",
      "step : 920 \tcost : 0.00074994424 \tbias : [0.07212891] \tweight : [[0.96827036]]\n",
      "step : 940 \tcost : 0.0006811122 \tbias : [0.06873915] \tweight : [[0.9697615]]\n",
      "step : 960 \tcost : 0.0006185979 \tbias : [0.06550866] \tweight : [[0.97118264]]\n",
      "step : 980 \tcost : 0.00056182063 \tbias : [0.06243001] \tweight : [[0.9725369]]\n",
      "step : 1000 \tcost : 0.0005102526 \tbias : [0.05949603] \tweight : [[0.9738276]]\n",
      "step : 1020 \tcost : 0.0004634207 \tbias : [0.05669993] \tweight : [[0.9750576]]\n",
      "step : 1040 \tcost : 0.00042088534 \tbias : [0.05403522] \tweight : [[0.97622985]]\n",
      "step : 1060 \tcost : 0.00038225626 \tbias : [0.05149576] \tweight : [[0.9773469]]\n",
      "step : 1080 \tcost : 0.0003471718 \tbias : [0.04907567] \tweight : [[0.9784115]]\n",
      "step : 1100 \tcost : 0.00031530656 \tbias : [0.04676931] \tweight : [[0.97942615]]\n",
      "step : 1120 \tcost : 0.00028636312 \tbias : [0.04457127] \tweight : [[0.98039305]]\n",
      "step : 1140 \tcost : 0.00026008004 \tbias : [0.04247656] \tweight : [[0.9813145]]\n",
      "step : 1160 \tcost : 0.00023620897 \tbias : [0.04048032] \tweight : [[0.98219264]]\n",
      "step : 1180 \tcost : 0.00021453059 \tbias : [0.0385779] \tweight : [[0.9830295]]\n",
      "step : 1200 \tcost : 0.0001948383 \tbias : [0.03676487] \tweight : [[0.98382705]]\n",
      "step : 1220 \tcost : 0.00017695712 \tbias : [0.03503707] \tweight : [[0.98458713]]\n",
      "step : 1240 \tcost : 0.00016071474 \tbias : [0.03339045] \tweight : [[0.9853115]]\n",
      "step : 1260 \tcost : 0.00014596382 \tbias : [0.03182122] \tweight : [[0.9860018]]\n",
      "step : 1280 \tcost : 0.00013256578 \tbias : [0.03032574] \tweight : [[0.98665965]]\n",
      "step : 1300 \tcost : 0.00012039869 \tbias : [0.02890054] \tweight : [[0.98728657]]\n",
      "step : 1320 \tcost : 0.00010934887 \tbias : [0.02754232] \tweight : [[0.9878841]]\n",
      "step : 1340 \tcost : 9.93123e-05 \tbias : [0.02624794] \tweight : [[0.98845345]]\n",
      "step : 1360 \tcost : 9.019696e-05 \tbias : [0.02501441] \tweight : [[0.9889961]]\n",
      "step : 1380 \tcost : 8.191908e-05 \tbias : [0.02383886] \tweight : [[0.98951316]]\n",
      "step : 1400 \tcost : 7.4400064e-05 \tbias : [0.02271854] \tweight : [[0.9900061]]\n",
      "step : 1420 \tcost : 6.757009e-05 \tbias : [0.02165085] \tweight : [[0.99047583]]\n",
      "step : 1440 \tcost : 6.136819e-05 \tbias : [0.0206333] \tweight : [[0.99092346]]\n",
      "step : 1460 \tcost : 5.573575e-05 \tbias : [0.01966357] \tweight : [[0.99135]]\n",
      "step : 1480 \tcost : 5.062057e-05 \tbias : [0.01873944] \tweight : [[0.9917565]]\n",
      "step : 1500 \tcost : 4.597423e-05 \tbias : [0.01785876] \tweight : [[0.99214387]]\n",
      "step : 1520 \tcost : 4.1754534e-05 \tbias : [0.01701947] \tweight : [[0.9925131]]\n",
      "step : 1540 \tcost : 3.7922408e-05 \tbias : [0.01621961] \tweight : [[0.99286497]]\n",
      "step : 1560 \tcost : 3.4441568e-05 \tbias : [0.01545734] \tweight : [[0.9932003]]\n",
      "step : 1580 \tcost : 3.1279596e-05 \tbias : [0.01473088] \tweight : [[0.9935199]]\n",
      "step : 1600 \tcost : 2.840843e-05 \tbias : [0.01403857] \tweight : [[0.9938244]]\n",
      "step : 1620 \tcost : 2.5801832e-05 \tbias : [0.01337879] \tweight : [[0.99411464]]\n",
      "step : 1640 \tcost : 2.3433206e-05 \tbias : [0.01275007] \tweight : [[0.9943912]]\n",
      "step : 1660 \tcost : 2.1282929e-05 \tbias : [0.01215087] \tweight : [[0.99465483]]\n",
      "step : 1680 \tcost : 1.9329502e-05 \tbias : [0.01157982] \tweight : [[0.994906]]\n",
      "step : 1700 \tcost : 1.7555647e-05 \tbias : [0.01103561] \tweight : [[0.99514544]]\n",
      "step : 1720 \tcost : 1.5943262e-05 \tbias : [0.01051697] \tweight : [[0.99537355]]\n",
      "step : 1740 \tcost : 1.4480425e-05 \tbias : [0.0100227] \tweight : [[0.995591]]\n",
      "step : 1760 \tcost : 1.3151684e-05 \tbias : [0.00955167] \tweight : [[0.9957982]]\n",
      "step : 1780 \tcost : 1.1944352e-05 \tbias : [0.0091028] \tweight : [[0.99599564]]\n",
      "step : 1800 \tcost : 1.0848035e-05 \tbias : [0.00867503] \tweight : [[0.9961838]]\n",
      "step : 1820 \tcost : 9.852422e-06 \tbias : [0.00826733] \tweight : [[0.99636316]]\n",
      "step : 1840 \tcost : 8.948102e-06 \tbias : [0.00787881] \tweight : [[0.99653405]]\n",
      "step : 1860 \tcost : 8.127165e-06 \tbias : [0.00750855] \tweight : [[0.996697]]\n",
      "step : 1880 \tcost : 7.3810197e-06 \tbias : [0.00715565] \tweight : [[0.9968523]]\n",
      "step : 1900 \tcost : 6.7037395e-06 \tbias : [0.00681934] \tweight : [[0.99700016]]\n",
      "step : 1920 \tcost : 6.087879e-06 \tbias : [0.00649885] \tweight : [[0.9971412]]\n",
      "step : 1940 \tcost : 5.5291234e-06 \tbias : [0.00619342] \tweight : [[0.99727553]]\n",
      "step : 1960 \tcost : 5.021602e-06 \tbias : [0.00590235] \tweight : [[0.9974035]]\n",
      "step : 1980 \tcost : 4.5606e-06 \tbias : [0.00562497] \tweight : [[0.9975256]]\n",
      "step : 2000 \tcost : 4.142329e-06 \tbias : [0.00536061] \tweight : [[0.99764186]]\n",
      "Tensor = 5 : [[4.99357]]\n",
      "Tensor = 2.5 : [[2.4994652]]\n",
      "Tensor = [1.5], [3.5] : [[1.5018234]\n",
      " [3.497107 ]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(777)   # for reproducibility\n",
    "\n",
    "x_train = [[1], [2], [3]]\n",
    "y_train = [[1], [2], [3]]\n",
    "X = Variable(torch.Tensor(x_train))\n",
    "Y = Variable(torch.Tensor(y_train))\n",
    "\n",
    "model = nn.Linear(1, 1, bias=True)\n",
    "\n",
    "criterion = nn.MSELoss() # Mean Square Error 비용\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # SGD (Stochastic Gradient Descent) : 확률적 경사 하강법\n",
    "                                                         # lr (Learning rate) : 학습률\n",
    "\n",
    "\n",
    "for step in range(2001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 20 == 0:\n",
    "        print(\"step :\", step, \"\\tcost :\", cost.data.numpy(), \"\\tbias :\", model.bias.data.numpy(), \"\\tweight :\", model.weight.data.numpy())\n",
    "\n",
    "        \n",
    "predicted = model(Variable(torch.Tensor([[5]])))\n",
    "print(\"Tensor = 5 :\", predicted.data.numpy())\n",
    "predicted = model(Variable(torch.Tensor([[2.5]])))\n",
    "print(\"Tensor = 2.5 :\", predicted.data.numpy())\n",
    "predicted = model(Variable(torch.Tensor([[1.5], [3.5]])))\n",
    "print(\"Tensor = [1.5], [3.5] :\", predicted.data.numpy())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
